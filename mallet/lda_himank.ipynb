{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"lda-himank.ipynb\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/12zx-jsZowxlpoN5yMB7CbRrZWVJVBfMu\n",
    "\"\"\"\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "import csv\n",
    "import random\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.test.utils import datapath\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "import json\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sampled_te_docs_10k.json', 'r') as outfile:  \n",
    "    test_docs = json.load(outfile)\n",
    "with open('sampled_tr_docs_10k.json', 'r') as outfile:  \n",
    "    train_docs = json.load(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sampled_tr_docs_10k_not_shuffled.json', 'r') as outfile:  \n",
    "    train_docs_unshuffled = json.load(outfile)\n",
    "with open('sampled_tr_docs_10k_trustworthy.json', 'r') as outfile:  \n",
    "    train_docs_trustworthy = json.load(outfile)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('data/sampled_all_docs_all.json', 'r') as outfile:\n",
    "    docs = json.load(outfile)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tr_docs_15k = []\n",
    "tr_docs_14k = []\n",
    "te_docs_14k = []\n",
    "tr_docs_13k = []\n",
    "te_docs_13k = []\n",
    "tr_docs_10k = []\n",
    "te_docs_10k = []\n",
    "\n",
    "for k,v in docs.items():\n",
    "    v = [row[5] for row in v]\n",
    "    tr_docs_15k += v\n",
    "    tr_docs_14k += v[:14000]\n",
    "    te_docs_14k += v[14000:]\n",
    "    tr_docs_13k += v[:13000]\n",
    "    te_docs_13k += v[13000:]\n",
    "    tr_docs_10k += v[:10000]\n",
    "    te_docs_10k += v[10000:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def write(name, data):\n",
    "    with open('data/{}.json'.format(name), 'w') as outfile:\n",
    "        random.shuffle(data)\n",
    "        json.dump(data, outfile)\n",
    "        \n",
    "write('sampled_tr_docs_15k', tr_docs_15k)\n",
    "write('sampled_tr_docs_14k', tr_docs_14k)\n",
    "write('sampled_tr_docs_13k', tr_docs_13k)\n",
    "write('sampled_tr_docs_10k', tr_docs_10k)\n",
    "write('sampled_te_docs_14k', te_docs_14k)\n",
    "write('sampled_te_docs_13k', te_docs_13k)\n",
    "write('sampled_te_docs_10k', te_docs_10k)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def write(name, data):\n",
    "    with open('data/{}.json'.format(name), 'w') as outfile:\n",
    "        random.shuffle(data)\n",
    "        json.dump(data, outfile)\n",
    "        \n",
    "tr_docs_15k = []\n",
    "tr_docs_14k = []\n",
    "te_docs_14k = []\n",
    "tr_docs_13k = []\n",
    "te_docs_13k = []\n",
    "tr_docs_10k = []\n",
    "te_docs_10k = []\n",
    "\n",
    "for k,v in docs.items():\n",
    "    tr_docs_15k += v\n",
    "    tr_docs_14k += v[:14000]\n",
    "    te_docs_14k += v[14000:]\n",
    "    tr_docs_13k += v[:13000]\n",
    "    te_docs_13k += v[13000:]\n",
    "    tr_docs_10k += v[:10000]\n",
    "    te_docs_10k += v[10000:]\n",
    "    \n",
    "write('sampled_tr_docs_15k_all', tr_docs_15k)\n",
    "write('sampled_tr_docs_14k_all', tr_docs_14k)\n",
    "write('sampled_tr_docs_13k_all', tr_docs_13k)\n",
    "write('sampled_tr_docs_10k_all', tr_docs_10k)\n",
    "write('sampled_te_docs_14k_all', te_docs_14k)\n",
    "write('sampled_te_docs_13k_all', te_docs_13k)\n",
    "write('sampled_te_docs_10k_all', te_docs_10k) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def write(name, data):\n",
    "    with open('data/{}.json'.format(name), 'w') as outfile:\n",
    "        random.shuffle(data)\n",
    "        json.dump(data, outfile)\n",
    "        \n",
    "tr_docs_5k = []\n",
    "te_docs_5k = []\n",
    "for k,v in docs.items():\n",
    "    v = [row[5] for row in v]\n",
    "    tr_docs_5k += v[:5000]\n",
    "    te_docs_5k += v[5000:]\n",
    "write('sampled_tr_docs_5k', tr_docs_5k)\n",
    "write('sampled_te_docs_5k', te_docs_5k)\n",
    "\n",
    "tr_docs_5k = []\n",
    "te_docs_5k = []\n",
    "for k,v in docs.items():\n",
    "    tr_docs_5k += v[:5000]\n",
    "    te_docs_5k += v[5000:]\n",
    "write('sampled_tr_docs_5k_all', tr_docs_5k)\n",
    "write('sampled_te_docs_5k_all', te_docs_5k)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('data/train_lda_10k.json', 'r') as outfile:\n",
    "    docs_old = json.load(outfile)\n",
    "with open('data/sampled_tr_docs_7k_all.json', 'r') as outfile:\n",
    "    docs_all = json.load(outfile)\n",
    "with open('data/sampled_tr_docs_7k.json', 'r') as outfile:\n",
    "    docs_new = json.load(outfile)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('data/sampled_all_docs_all.json', 'r') as outfile:\n",
    "    docs = json.load(outfile)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ugghh = 0\n",
    "for k,v in docs.items():\n",
    "    for elem in v:\n",
    "        if len(elem) != 17:\n",
    "            print(k)\n",
    "            print(len(elem))\n",
    "            print(v)\n",
    "ugghh"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ugghh = 0\n",
    "for elem in docs_all:\n",
    "    if len(elem) != 17:\n",
    "        ugghh += 1\n",
    "print(ugghh)\n",
    "len(docs_all)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('data/sampled_tr_docs_10k.json', 'r') as outfile:\n",
    "    docs_old = json.load(outfile)\n",
    "with open('data/train_lda_10k.json', 'r') as outfile:\n",
    "    docs_all = json.load(outfile)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "type(docs_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(doc):\n",
    "    deacc = gensim.utils.simple_preprocess(doc, deacc=True, min_len=4)\n",
    "    stop_free = [i for i in deacc if i not in stop]\n",
    "    normalized = [lemma.lemmatize(word) for word in stop_free]\n",
    "    return normalized\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_docs_clean = [clean(doc) for doc in train_docs]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### SAVING dict\n",
    "dictionary = corpora.Dictionary(train_docs_clean)\n",
    "dictionary.save_as_text('dict_10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD dict\n",
    "dictionary = corpora.Dictionary.load_from_text('dict_10k')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in train_docs_clean]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_docs_clean = [clean(doc) for doc in test_docs]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_doc_term_matrix = [dictionary.doc2bow(text) for text in test_docs_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('doc_term_matrix.json', 'r') as outfile:\n",
    "    doc_term_matrix = json.load(outfile)\n",
    "with open('test_doc_term_matrix.json', 'r') as outfile:\n",
    "    test_doc_term_matrix = json.load(outfile)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Lda = gensim.models.wrappers.LdaMallet\n",
    "# ldamodel = Lda(doc_term_matrix, num_topics=100, id2word = dictionary, passes=50)\n",
    "ldamodel = Lda('Mallet/bin/mallet', corpus=doc_term_matrix, num_topics=100, id2word = dictionary, optimize_interval=10)\n",
    "temp_file = \"model____\"\n",
    "ldamodel.save(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### LOAD MODEL\n",
    "Lda = gensim.models.wrappers.LdaMallet\n",
    "ldamodel_10k = Lda.load(\"model_10k_5000_alpha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(test_docs)\n",
    "len(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DOC_NUM_FROM_TESTING = 16000  # range is 0 - 184706\n",
    "DOC_NUM_FROM_TRAINING = 500   # range is 0 - 70000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As Americans begin the Memorial Day weekend, we remember those who have given their lives to defend the freedoms and way of life that we enjoy. The Heritage Foundation’s James Carafano writes in The Sacramento Bee that as we honor them, we must also “do our utmost not to add to their ranks”:\n",
      "\n",
      "Cold gray monuments, brassy parades, majestic flyovers – they are all remembrances of those who died in the service of the nation. They are all part of our Memorial Day. No day speaks more about American patriotism than the day we thank those who gave their lives in the fight for freedom. Yet, no ceremony, no solemnity can ever replace those we have lost . . . So while on this day we honor sacrifice, we have a job the rest of the year as well: reminding our leaders in Washington to ensure that the troops who defend us have what they need to do the job – and come back to us. There is no better way to recognize the valor of those who serve, and demonstrate care and respect for their families, than to pay it forward – to properly arm our armed forces for the next fight.\n",
      "\n",
      "Carafano writes that adequately funding defense is among America’s greatest challenges, and it is one that must be addressed:\n",
      "\n",
      "After 10 years, we have put a lot of wear and tear on the armed forces. The danger that our military preparedness could plummet has never been greater. Today, America has the smallest Navy since before World War I, and the force is aging. This year marks the 25th anniversary of the popular movie “Top Gun.” The ship featured in the film was the USS Enterprise. It is still at sea. In fact, it was commissioned in the 1960s, and is the second oldest ship in the U.S. fleet. Ships in the Navy’s sister fleet, the U.S. Coast Guard, are even older . . .\n",
      "\n",
      "America’s Air Force has the oldest average fleet of planes and the fewest number of planes in its inventory at any time since World War II . . .\n"
     ]
    }
   ],
   "source": [
    "print(test_docs[DOC_NUM_FROM_TESTING])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.269611183791489 [('military', 0.050022350411886164), ('force', 0.031943350575079646), ('defense', 0.020636880308222822), ('army', 0.01681602418101705), ('soldier', 0.011086513832421579), ('missile', 0.010100257561889354), ('troop', 0.009614224795475993), ('weapon', 0.009380077623335249), ('navy', 0.008950807807743886), ('operation', 0.008929521701185637)]\n",
      "\n",
      "\n",
      "0.2030686363031811 [('american', 0.02050535392062583), ('world', 0.01931544371089909), ('people', 0.019300586716226), ('government', 0.018875136414223816), ('america', 0.017171984570335714), ('state', 0.013565786772412452), ('country', 0.011732973884104636), ('nation', 0.011507417692249512), ('united', 0.008934456342045835), ('power', 0.00876967876476245)]\n",
      "\n",
      "\n",
      "0.13758735018774282 [('protest', 0.02107634585819354), ('event', 0.016034035594281407), ('people', 0.015471886139183095), ('protester', 0.013321238602632733), ('group', 0.012320442224238011), ('flag', 0.01166034248529681), ('march', 0.010438093291257299), ('rally', 0.009752441304357084), ('king', 0.008372619914570318), ('street', 0.007772142087533484)]\n",
      "\n",
      "\n",
      "0.0802958447309429 [('child', 0.06591315850906544), ('family', 0.05024900785931056), ('year', 0.022072212279200063), ('mother', 0.019208621897128627), ('parent', 0.019119134697688896), ('life', 0.016625165356781574), ('home', 0.016333359271652012), ('father', 0.01555910045910824), ('woman', 0.0126099136253988), ('wife', 0.011411563302466734)]\n",
      "\n",
      "\n",
      "0.07841426238347844 [('people', 0.03722342977592707), ('thing', 0.020629216331550952), ('time', 0.017028463318795744), ('make', 0.015820779208585472), ('life', 0.012562731502128999), ('good', 0.0118737999446038), ('work', 0.00841975297050387), ('feel', 0.007216763453530569), ('find', 0.006395209637060997), ('world', 0.006273150212899804)]\n",
      "\n",
      "\n",
      "0.05613463272908542 [('percent', 0.05905304486904724), ('year', 0.04499396243236822), ('rate', 0.023711842695033028), ('million', 0.02040526407316601), ('number', 0.019704759268829742), ('increase', 0.013243459709951822), ('report', 0.01214371615349383), ('population', 0.011350137284245745), ('month', 0.010110782630420033), ('data', 0.009878097468140503)]\n",
      "\n",
      "\n",
      "0.03182613509943349 [('issue', 0.010244002700313087), ('policy', 0.007932612659223678), ('time', 0.006680794288944131), ('problem', 0.006625866660524222), ('make', 0.005489219124028357), ('effort', 0.005403283963435918), ('action', 0.005152565917583752), ('interest', 0.005145478481658602), ('decision', 0.005013474987552691), ('change', 0.004852235820255538)]\n",
      "\n",
      "\n",
      "0.02482070508789909 [('program', 0.019565265026184893), ('project', 0.012901812126125732), ('group', 0.012855796831518286), ('organization', 0.010776343756162763), ('member', 0.009722374389201744), ('work', 0.008221399303196967), ('development', 0.008109647873436028), ('plan', 0.007943116331047176), ('national', 0.0075925236102285425), ('community', 0.007362447137191314)]\n",
      "\n",
      "\n",
      "0.02113449265300474 [('film', 0.01711610738492326), ('show', 0.012855642729101625), ('movie', 0.011604020747225394), ('music', 0.008980564502727759), ('star', 0.008114328950299293), ('year', 0.008114328950299293), ('photo', 0.007838547835648598), ('song', 0.006388929156074433), ('award', 0.005706547680079764), ('hollywood', 0.005324696905948033)]\n",
      "\n",
      "\n",
      "0.02018793277188845 [('fact', 0.0106188420960851), ('time', 0.00631593861714186), ('point', 0.00604891446507978), ('make', 0.005253291481384602), ('word', 0.005048391315720638), ('question', 0.0047715581131746445), ('real', 0.004736681489231842), ('claim', 0.0043312407358967655), ('medium', 0.004294184322957538), ('reason', 0.004247318859534397)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tops = ldamodel_10k[test_doc_term_matrix[DOC_NUM_FROM_TESTING]]\n",
    "tops = sorted(tops, key=lambda a:a[1], reverse=True)\n",
    "for top in tops[:10]:\n",
    "    print(top[1], ldamodel_10k.show_topic(top[0]))\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix_unshuffled = [dictionary.doc2bow(doc) for doc in [clean(d) for d in train_docs_unshuffled]]\n",
    "doc_term_matrix_trustworthy = [dictionary.doc2bow(doc) for doc in [clean(d) for d in train_docs_trustworthy]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_docs[DOC_NUM_FROM_TRAINING])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In truth, there is only one powerplant a Ford Mustang should be purchased with and that's the V-8. If an Internet forum is to be believed, it sounds like Ford somewhat agrees – the automaker is doing away with its entry-level V-6, leaving the Ecoboost four-cylinder and the torquey V-8. Mustang6g forum user XKRJAG claims some snooping through the vehicle ordering system uncovered that the option numbers for the 300-horsepower V-6 -engined Mustangs for the 2018 model year no longer work, which could mean the entry-level power plant has been discontinued. If this is true, it would mean for that for the non-Shelby Mustangs, customers would only be able to choose from the 310-horsepower turbocharged four-cylinder motor and the 435-horsepower V-8. Both are quality options and, at least for us at The Drive , tears for the nixed sixer would not be shed.\n",
      "Additionally, XKRJAG noticed there are new performance packs for the GT trim level, which could hint that a track-friendly, Chevy Camaro SS 1LE fighter is in the works. Other details from the post show an optional 10-speed automatic will be available for the 2018 model year as well. Currently, the Mustang is available with an optional six-speed auto.\n",
      "When asked by Jalopnik if this post reflected actual changes for the Mustang's Future, a Ford spokesperson said the company does not talk about future products. But, if priced appropriately, a new 1LE fighting-Mustang could be incredibly well-received by the Pony car community. Maybe something along the lines of a new Boss 302 Laguna Seca edition?\n",
      "So, Ford, why not just kill the V-6? The four-cylinder \" Boostang \" is a perfectly viable option for those who don't want to fork over the extra scratch for the V-8, and we doubt the manufacturer would see any kind of backlash from its loving enthusiast community following the loss.\n",
      "Go for it.\n"
     ]
    }
   ],
   "source": [
    "print(train_docs_unshuffled[5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In just the last week, we've seen new chatter about the possibility that Donald Trump could put the House in play for Democrats, as non-partisan analysts, giddy liberals, and even some anguished Republicans have started wondering whether Trump could pull off such a remarkable feat. But while it's certainly not impossible to imagine, any such conclusions are very premature. There's still far too much we don't know—and won't know—unless and until Trump is the nominee (and of course, he may not be). Democrats have certainly bet wrong before on the effects a supposedly \"unacceptable\" Republican standard-bearer might have for his party.\n",
      "\n",
      "But one thing we do know does not augur well for Democrats: Because a House takeover has seemed so remote this cycle, the party has struggled to land strong candidates in a large enough number of potentially competitive districts to even contemplate overcoming its 30-seat deficit. The prospect of a Trumpocalypse has only started to feel real quite recently, and it's somewhat late in the game to mount a serious campaign against an incumbent. What's more, filing deadlines have passed in 37 percent of the nation's 435 House districts, so even if legitimate contenders wanted to take a second look, in many cases, they couldn't.\n",
      "\n",
      "Two GOP-held seats in southern New Jersey illustrate the problem. In the 2nd District, Republican Rep. Frank LoBiondo has been entrenched for many years, but Barack Obama carried the seat by a 54-45 margin, and it's the kind of place where Democrats have to set themselves up to be able to take advantage of a potential Trump wave. But the candidate that Democrats in the district's largest county endorsed over the weekend is former Obama aide Dave Cole, who ran in 2014, raised just $55,000, and lost the primary 82-18.\n",
      "\n",
      "Meanwhile, in the neighboring 3rd District, Democrats there are split between two bantamweights: Jim Keady, who badly lost a race for state Assembly last year, and Frederick LaVergne, who took 1.7 percent in the general election here in 2014 on the \"Democratic-Republican\" line. (And you thought the Jeffersonians were dead!) While this seat isn't quite as blue (Obama won it 52-47), GOP Rep. Tom MacArthur is only in his first term, when incumbents are at their most vulnerable, and Democrats actually won here in 2008.\n",
      "\n",
      "New Jersey's filing deadline is just two weeks away, so no matter what new outrages Trump provokes in the next fortnight, the recruitment situation in the Garden State is unlikely to improve. And it's pretty much the same story everywhere else. If Trump were to truly cause a cataclysm, he might sweep in a few very unheralded Democrats on his anti-coattails, and perhaps that would be enough for a miracle. But while Republicans have every reason to feel queasy about what The Donald might mean for their future, for now, their firewall in the House of Representatives looks to be quite operational.\n"
     ]
    }
   ],
   "source": [
    "print(train_docs_unshuffled[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4932227433128682 [('republican', 0.041565425076621476), ('candidate', 0.020566998923157633), ('party', 0.02026327415302206), ('campaign', 0.018737747466659304), ('democrat', 0.018113040837175912), ('conservative', 0.015351906563216168), ('romney', 0.011576055443576221), ('democratic', 0.011444901565563133), ('mccain', 0.009305022503244333), ('election', 0.00901510340447856)]\n",
      "\n",
      "\n",
      "0.1387479528261895 [('issue', 0.010244002700313087), ('policy', 0.007932612659223678), ('time', 0.006680794288944131), ('problem', 0.006625866660524222), ('make', 0.005489219124028357), ('effort', 0.005403283963435918), ('action', 0.005152565917583752), ('interest', 0.005145478481658602), ('decision', 0.005013474987552691), ('change', 0.004852235820255538)]\n",
      "\n",
      "\n",
      "0.12098641767144316 [('fact', 0.0106188420960851), ('time', 0.00631593861714186), ('point', 0.00604891446507978), ('make', 0.005253291481384602), ('word', 0.005048391315720638), ('question', 0.0047715581131746445), ('real', 0.004736681489231842), ('claim', 0.0043312407358967655), ('medium', 0.004294184322957538), ('reason', 0.004247318859534397)]\n",
      "\n",
      "\n",
      "0.07210317081785196 [('trump', 0.1660354861219319), ('donald', 0.03840265028081324), ('president', 0.03537433861537732), ('election', 0.012611190526194558), ('campaign', 0.011320601326495356), ('white', 0.01009098484489218), ('elect', 0.007946777591848624), ('house', 0.00769611197300941), ('presidential', 0.0071880059888758665), ('republican', 0.006893304518078411)]\n",
      "\n",
      "\n",
      "0.0274001949387921 [('bill', 0.0383416527601138), ('house', 0.032030405728274536), ('senate', 0.028523748961603212), ('republican', 0.025594183507685973), ('congress', 0.023697501231373184), ('senator', 0.017342145309387106), ('democrat', 0.015379299697854102), ('committee', 0.01489777764708477), ('vote', 0.014555933748446999), ('legislation', 0.010211208068986304)]\n",
      "\n",
      "\n",
      "0.02491369150511648 [('election', 0.05722456985299781), ('vote', 0.05665976925328226), ('voter', 0.04513270246817862), ('poll', 0.02663291555204124), ('state', 0.02170888123270298), ('voting', 0.021236502549304526), ('candidate', 0.016132758948238592), ('ballot', 0.013914632956628448), ('republican', 0.012215096606575306), ('percent', 0.011809467084961413)]\n",
      "\n",
      "\n",
      "0.02078966615739141 [('state', 0.0699147107140446), ('county', 0.03247743997119518), ('california', 0.024718140288498322), ('texas', 0.020712469338614217), ('photo', 0.014370907126943763), ('image', 0.010459752008461417), ('florida', 0.01029322411503927), ('carolina', 0.009667619326237145), ('governor', 0.009091522830074037), ('north', 0.008938497198280711)]\n",
      "\n",
      "\n",
      "0.017154794017596165 [('obama', 0.1442414230745986), ('president', 0.07789036846028324), ('administration', 0.025109289067064205), ('barack', 0.02269183504905417), ('american', 0.021084228127077498), ('house', 0.018022119704264792), ('white', 0.017526541630572736), ('america', 0.010846310360805012), ('washington', 0.0072040129736698965), ('bush', 0.006446544048026753)]\n",
      "\n",
      "\n",
      "0.012844278557528612 [('people', 0.03722342977592707), ('thing', 0.020629216331550952), ('time', 0.017028463318795744), ('make', 0.015820779208585472), ('life', 0.012562731502128999), ('good', 0.0118737999446038), ('work', 0.00841975297050387), ('feel', 0.007216763453530569), ('find', 0.006395209637060997), ('world', 0.006273150212899804)]\n",
      "\n",
      "\n",
      "0.01214110308952576 [('team', 0.01604789600895061), ('game', 0.01260601091496067), ('world', 0.011273668297932306), ('player', 0.010590415673815197), ('sport', 0.010018191601117119), ('year', 0.009112881874161947), ('match', 0.008865202797919495), ('league', 0.007242477815641361), ('club', 0.006465277955708148), ('final', 0.006371330719892046)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tops = ldamodel_10k[doc_term_matrix_unshuffled[100]]\n",
    "tops = sorted(tops, key=lambda a:a[1], reverse=True)\n",
    "for top in tops[:10]:\n",
    "    print(top[1], ldamodel_10k.show_topic(top[0]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31787755521408834 [('military', 0.050022350411886164), ('force', 0.031943350575079646), ('defense', 0.020636880308222822), ('army', 0.01681602418101705), ('soldier', 0.011086513832421579), ('missile', 0.010100257561889354), ('troop', 0.009614224795475993), ('weapon', 0.009380077623335249), ('navy', 0.008950807807743886), ('operation', 0.008929521701185637)]\n",
      "\n",
      "\n",
      "0.206582285139536 [('attack', 0.03996745321399512), ('terrorist', 0.030124375217947228), ('terrorism', 0.016068813204696036), ('security', 0.013437173079158433), ('terror', 0.013423224456584911), ('group', 0.012302685109845403), ('killed', 0.01049401371614553), ('state', 0.01032663024526328), ('bomb', 0.00823898640009299), ('islamic', 0.00806230384749506)]\n",
      "\n",
      "\n",
      "0.1517611207159785 [('israel', 0.07646042066546745), ('iran', 0.04999547779597825), ('israeli', 0.040157171713111374), ('palestinian', 0.02898732777939684), ('iranian', 0.018983207549065913), ('state', 0.014435880171642766), ('jewish', 0.012074284738063894), ('arab', 0.012069260066928619), ('nuclear', 0.009948848847842908), ('peace', 0.009139876795063763)]\n",
      "\n",
      "\n",
      "0.07006193113950354 [('syria', 0.040706558928957644), ('syrian', 0.028208464666735215), ('isi', 0.01569264261123304), ('force', 0.014671521718319548), ('iraq', 0.012150629513939365), ('state', 0.012136447279315565), ('assad', 0.010952230688228391), ('group', 0.010789134990054707), ('government', 0.009789287449076913), ('rebel', 0.009651010661494878)]\n",
      "\n",
      "\n",
      "0.059931898970306305 [('saudi', 0.053283410138248846), ('turkey', 0.028971700342668084), ('arabia', 0.02601028004253811), ('libya', 0.02488036157390996), ('egypt', 0.022797766749379653), ('turkish', 0.01770944109653787), ('egyptian', 0.014334455866713931), ('arab', 0.013019910197329553), ('libyan', 0.012480798771121352), ('country', 0.01228878648233487)]\n",
      "\n",
      "\n",
      "0.05950365712112604 [('issue', 0.010244002700313087), ('policy', 0.007932612659223678), ('time', 0.006680794288944131), ('problem', 0.006625866660524222), ('make', 0.005489219124028357), ('effort', 0.005403283963435918), ('action', 0.005152565917583752), ('interest', 0.005145478481658602), ('decision', 0.005013474987552691), ('change', 0.004852235820255538)]\n",
      "\n",
      "\n",
      "0.026343571436174638 [('immigration', 0.048930544476941715), ('immigrant', 0.03922937162102278), ('illegal', 0.027980885698248102), ('border', 0.02635232946320423), ('snip', 0.01809137640698396), ('country', 0.0153032585896186), ('state', 0.015081686312741883), ('refugee', 0.01361930928535555), ('alien', 0.010713019586989276), ('united', 0.010173860379922597)]\n",
      "\n",
      "\n",
      "0.018737500061530517 [('vehicle', 0.01263321955020909), ('car', 0.007567858279643065), ('system', 0.006662349493463873), ('truck', 0.005987333852857566), ('design', 0.005833671755808976), ('make', 0.005098288862790723), ('driver', 0.004889747445367637), ('equipment', 0.004725109484244147), ('electric', 0.004659254299794751), ('engine', 0.004653766367757301)]\n",
      "\n",
      "\n",
      "0.0136960817105259 [('people', 0.03722342977592707), ('thing', 0.020629216331550952), ('time', 0.017028463318795744), ('make', 0.015820779208585472), ('life', 0.012562731502128999), ('good', 0.0118737999446038), ('work', 0.00841975297050387), ('feel', 0.007216763453530569), ('find', 0.006395209637060997), ('world', 0.006273150212899804)]\n",
      "\n",
      "\n",
      "0.01055598515088782 [('state', 0.03201466841493705), ('court', 0.031430490261920754), ('government', 0.016676793842676986), ('federal', 0.014939183643011715), ('constitution', 0.011621733985736672), ('legal', 0.0115833573187502), ('supreme', 0.010116515825045039), ('law', 0.010073875083948958), ('case', 0.009982197490592386), ('justice', 0.009131514705725586)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##SHOULD THIS BE doc_term_matrix, you had test_doc_term_matrix??\n",
    "tops = ldamodel_10k[doc_term_matrix[DOC_NUM_FROM_TRAINING]]\n",
    "tops = sorted(tops, key=lambda a:a[1], reverse=True)\n",
    "for top in tops[:10]:\n",
    "    print(top[1], ldamodel_10k.show_topic(top[0]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below This Is Katie Sketching Out Comparison Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "K = ldamodel_10k.num_topics\n",
    "print(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(ldamodel_10k[doc_term_matrix[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unshuffled Matrix => 0-20k -> trustworthy; 20k-70k -> untrustworthy\n",
    "THRESH = 0.1\n",
    "all_trusted_tops = ldamodel_10k[doc_term_matrix_unshuffled[:20000]]\n",
    "all_trusted_tops_lines = list(map(lambda x: [round(v, 3) if v > THRESH else 0 for t, v in x], all_trusted_tops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Feel free to look at the output\n",
    "######## Uncomment whatever you want to see\n",
    "\n",
    "# UNSHUFFLED_DOC = 100\n",
    "\n",
    "# # prints doc\n",
    "# print(train_docs_unshuffled[UNSHUFFLED_DOC])\n",
    "\n",
    "# # prints english topics\n",
    "# topics = ldamodel_10k[doc_term_matrix_unshuffled[UNSHUFFLED_DOC]]\n",
    "# tops = sorted(topics, key=lambda a:a[1], reverse=True)\n",
    "# for top in tops[:10]:\n",
    "#     print(top[1], ldamodel_10k.show_topic(top[0]))\n",
    "#     print(\"\\n\")\n",
    "    \n",
    "# # prints wihtout rounding\n",
    "# print(all_trusted_tops[UNSHUFFLED_DOC])\n",
    "\n",
    "# # prints with rounding as is written as a row of csv\n",
    "# print(all_trusted_tops_lines[UNSHUFFLED_DOC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Matrices/topic_doc_matrix.csv', 'w') as writeFile:\n",
    "    writer = csv.writer(writeFile)\n",
    "    writer.writerows(all_trusted_tops_lines)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "f = open('matrices/topic_doc_matrix.csv', 'w')\n",
    "writer = csv.writer(f)\n",
    "for i in range(len(train_docs)):\n",
    "    print(i)\n",
    "    if(i % 10000 == 0):\n",
    "        f.close()\n",
    "        f = open('matrices/topic_doc_matrix' + str(i) + '.csv', 'w')\n",
    "        writer = csv.writer(f)\n",
    "    t0 = time.clock()\n",
    "    tops = ldamodel_10k[doc_term_matrix[i]]\n",
    "    print(t0-time.clock())\n",
    "    writer.writerow([round(v, 3) if v > .1 else 0 for t, v in tops])\n",
    "    print(t0-time.clock())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.matutils as mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_topic_matrix = ldamodel_10k.get_topics()\n",
    "K = 100\n",
    "topic_comps = np.zeros((K,K))\n",
    "ind = list(range(term_topic_matrix.shape[1]))\n",
    "for i1 in range(K):\n",
    "    print('!!!' + str(i1))\n",
    "    a = list(term_topic_matrix[i1, :])\n",
    "    #print('A')\n",
    "    a = list(zip(ind, a))\n",
    "    #print('B')\n",
    "    #a = [(1,4)]\n",
    "    #print(a)\n",
    "    for j1 in range(i1, K):\n",
    "        #print(j1)\n",
    "        b = list(term_topic_matrix[j1, :])\n",
    "        #print('a')\n",
    "        b = list(zip(ind, b))\n",
    "        #print('b')\n",
    "        sim = mt.cossim(a, b)\n",
    "        #print('c')\n",
    "        topic_comps[i1, j1] = sim\n",
    "        #print('d')\n",
    "        topic_comps[j1, i1] = sim  \n",
    "        #print('e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"topic_sims.csv\", topic_comps, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs(top, matches):\n",
    "    tot_length = 0\n",
    "    docs = []\n",
    "    scores = []\n",
    "    while(tot_length < 5000 and len(docs) < 3):\n",
    "        doc = matches[top, 'Match_' + str(len(docs) + 1)]\n",
    "        docs.append(doc)\n",
    "        scores.append(matches[top, 'Val_' + str(len(docs) + 1)])\n",
    "        tot_length += len(doc.split()) \n",
    "    return docs, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do sampling for documents to make summary for\n",
    "nums = [random.randing(0, 9999) for i in range(5*30)]\n",
    "type_nums = [10000*(i%30+2) for i in range(30*5)]\n",
    "sampled_doc_nums = tuple(map(sum, zip(nums, type_nums)))\n",
    "# Unshuffled Matrix => 0-20k -> trustworthy; 20k-70k -> untrustworthy\n",
    "\n",
    "full_documents = # load this some how    sampled_tr_docs_10k_not_shuffled.json\n",
    "matches = #load this somehow    topics_match_indices.csv\n",
    "\n",
    "THRESH = 0.1\n",
    "all_untrusted_tops = ldamodel_10k[doc_term_matrix_unshuffled[sampled_doc_nums]]\n",
    "all_untrusted_tops_lines = list(map(lambda x: [round(v, 3) if v > THRESH else 0 for t, v in x], all_trusted_tops))\n",
    "\n",
    "SUM_THRESH = .15\n",
    "doc_matches_matrix = []\n",
    "for doc in all_untrusted_tops_lines:\n",
    "    matched_documents = [full_documents[doc]]\n",
    "    for top in doc:\n",
    "        if(top < SUM_THRESH):\n",
    "            continue\n",
    "        topic_docs, scores = get_docs(top, matches)\n",
    "        matched_documents.append({'documents': topic_docs, 'scores': scores})\n",
    "    with open('matches/' + str(doc) +'.json', 'w') as fout:\n",
    "        json.dump(matched_documents, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Summarizing done in a separate script"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
