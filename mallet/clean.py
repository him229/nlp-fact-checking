# -*- coding: utf-8 -*-
"""lda-himank.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12zx-jsZowxlpoN5yMB7CbRrZWVJVBfMu
"""

import nltk
nltk.download('stopwords')
nltk.download('wordnet')
import re
import numpy as np
import pandas as pd
from pprint import pprint
import os
from pathlib import Path
import json
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.corpus import wordnet as wn
import csv
import random
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel
from gensim.test.utils import datapath
import spacy
from nltk.corpus import stopwords
stop_words = stopwords.words('english')
import json
import random

with open('sampled_te_docs_10k.json', 'r') as outfile:  
    test_docs = json.load(outfile)
with open('sampled_tr_docs_10k.json', 'r') as outfile:  
    train_docs = json.load(outfile)

def clean(doc):
    deacc = gensim.utils.simple_preprocess(doc, deacc=True, min_len=4)
    stop_free = [i for i in deacc if i not in stop]
    normalized = [lemma.lemmatize(word) for word in stop_free]
    return normalized

stop = set(stopwords.words('english'))
lemma = WordNetLemmatizer()

train_docs_clean = [clean(doc) for doc in train_docs]
test_docs_clean = [clean(doc) for doc in test_docs]

with open('train_docs_clean.json', 'w') as outfile:  
    json.dump(train_docs_clean, outfile)
with open('test_docs_clean.json', 'w') as outfile:  
    json.dump(test_docs_clean, outfile)

dictionary = corpora.Dictionary.load_from_text('dict_10k')

doc_term_matrix = [dictionary.doc2bow(doc) for doc in train_docs_clean]
test_doc_term_matrix = [dictionary.doc2bow(text) for text in test_docs_clean]

with open('doc_term_matrix.json', 'w') as outfile:  
    json.dump(doc_term_matrix, outfile)
with open('test_doc_term_matrix.json', 'w') as outfile:  
    json.dump(test_doc_term_matrix, outfile)